{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create crossval ext tables\n",
    "\n",
    "This notebook is dedicated to create the cv table to all tunings produced during 2020 for tracking purpose.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to JupyROOT 6.16/00\n",
      "Using all sub packages with ROOT dependence\n",
      "\n",
      "Applying ATLAS style settings...\n",
      "\n",
      "Applying ATLAS style settings...\n",
      "INFO: Pandarallel will run on 40 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n",
      "\n",
      "Applying ATLAS style settings...\n"
     ]
    }
   ],
   "source": [
    "from core import crossval_table, get_color_fader\n",
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "from sklearn.metrics import roc_curve\n",
    "import tensorflow as tf\n",
    "from Gaugi import mkdir_p\n",
    "from copy import copy\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "import pandas\n",
    "import collections\n",
    "import os\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "\n",
    "from tensorflow.keras.models import Model, model_from_json\n",
    "from Gaugi import load as gload\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir_p('output/crossval')\n",
    "#mkdir_p('output/training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_op_dict(op, decoration='reference'):\n",
    "    \n",
    "    d = collections.OrderedDict( {\n",
    "              # validation\n",
    "              \"max_sp_val\"      : 'summary/max_sp_val',\n",
    "              \"max_sp_pd_val\"   : 'summary/max_sp_pd_val#0',\n",
    "              \"max_sp_fa_val\"   : 'summary/max_sp_fa_val#0',\n",
    "              # Operation\n",
    "              \"max_sp_op\"       : 'summary/max_sp_op',\n",
    "              \"max_sp_pd_op\"    : 'summary/max_sp_pd_op#0',\n",
    "              \"max_sp_fa_op\"    : 'summary/max_sp_fa_op#0',\n",
    "              \n",
    "              # op\n",
    "              'pd_ref'    : decoration+\"/\"+op+\"/pd_ref#0\",\n",
    "              'fa_ref'    : decoration+\"/\"+op+\"/fa_ref#0\",\n",
    "              'sp_ref'    : decoration+\"/\"+op+\"/sp_ref\",\n",
    "              'pd_val'    : decoration+\"/\"+op+\"/pd_val#0\",\n",
    "              'fa_val'    : decoration+\"/\"+op+\"/fa_val#0\",\n",
    "              'sp_val'    : decoration+\"/\"+op+\"/sp_val\",\n",
    "              'pd_op'     : decoration+\"/\"+op+\"/pd_op#0\",\n",
    "              'fa_op'     : decoration+\"/\"+op+\"/fa_op#0\",\n",
    "              'sp_op'     : decoration+\"/\"+op+\"/sp_op\",\n",
    "\n",
    "              # Counts\n",
    "              'pd_ref_passed'    : decoration+\"/\"+op+\"/pd_ref#1\",\n",
    "              'fa_ref_passed'    : decoration+\"/\"+op+\"/fa_ref#1\",\n",
    "              'pd_ref_total'     : decoration+\"/\"+op+\"/pd_ref#2\",\n",
    "              'fa_ref_total'     : decoration+\"/\"+op+\"/fa_ref#2\",\n",
    "              'pd_val_passed'    : decoration+\"/\"+op+\"/pd_val#1\",\n",
    "              'fa_val_passed'    : decoration+\"/\"+op+\"/fa_val#1\",\n",
    "              'pd_val_total'     : decoration+\"/\"+op+\"/pd_val#2\",\n",
    "              'fa_val_total'     : decoration+\"/\"+op+\"/fa_val#2\",\n",
    "              'pd_op_passed'     : decoration+\"/\"+op+\"/pd_op#1\",\n",
    "              'fa_op_passed'     : decoration+\"/\"+op+\"/fa_op#1\",\n",
    "              'pd_op_total'      : decoration+\"/\"+op+\"/pd_op#2\",\n",
    "              'fa_op_total'      : decoration+\"/\"+op+\"/fa_op#2\",\n",
    "    })\n",
    "    return d\n",
    "\n",
    "\n",
    "op_names = ['tight', 'medium', 'loose', 'vloose']\n",
    "\n",
    "tuned_info = collections.OrderedDict({})\n",
    "for op in op_names:\n",
    "    tuned_info[op] = create_op_dict(op, \"reference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "etbins = [15, 20, 30, 40, 50,100, 1000000]\n",
    "etabins = [0.0, 0.8, 1.37, 1.54, 2.37, 2.50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Reading all tunings:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv  = crossval_table( tuned_info, etbins = etbins , etabins = etabins )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.from_csv('output/crossval/table_v12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_inits = cv.filter_inits(\"max_sp_val\")\n",
    "best_inits = best_inits.loc[best_inits.model_idx==3]\n",
    "best_sorts = cv.filter_sorts( best_inits , 'max_sp_op')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extended bins:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ext_table = best_inits.loc[best_inits.et_bin==4].reset_index(drop=True)\n",
    "ext_table['et_bin'] = 5\n",
    "best_inits = pandas.concat((best_inits, ext_table),axis=0,ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "basepath = '/home/jodafons/public/cern_data/new_files/data17_13TeV.AllPeriods.sgn.probes_lhvloose_EGAM1.bkg.vprobes_vlhvloose_EGAM7.GRL_v97.25bins'\n",
    "datapath = basepath+'/data17_13TeV.AllPeriods.sgn.probes_lhvloose_EGAM1.bkg.vprobes_vlhvloose_EGAM7.GRL_v97.25bins_et{ET}_eta{ETA}.h5'\n",
    "paths = [ [datapath.format(ET=et_bin, ETA=eta_bin) for eta_bin in range(5)] for et_bin in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-10 21:16:22.092427: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-10 21:16:22.092466: E tensorflow/stream_executor/cuda/cuda_driver.cc:314] failed call to cuInit: UNKNOWN ERROR (-1)\n",
      "2022-03-10 21:16:22.092491: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (caloba51): /proc/driver/nvidia/version does not exist\n",
      "2022-03-10 21:16:22.095404: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN)to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-03-10 21:16:22.188475: I tensorflow/core/platform/profile_utils/cpu_utils.cc:104] CPU Frequency: 2400000000 Hz\n",
      "2022-03-10 21:16:22.197768: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x8f3c0e0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
      "2022-03-10 21:16:22.197809: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n"
     ]
    }
   ],
   "source": [
    "# calculate all pd/fa from reference file\n",
    "ref_path = '/home/jodafons/public/cern_data/new_files/data17_13TeV.AllPeriods.sgn.probes_lhvloose_EGAM1.bkg.vprobes_vlhvloose_EGAM7.GRL_v97.25bins/'\n",
    "ref_path += 'new_references/data17_13TeV.AllPeriods.sgn.probes_lhmedium_EGAM1.bkg.vprobes_vlhvloose_EGAM7.GRL_v97.25bins_et{ET}_eta{ETA}.ref.npz'\n",
    "ref_paths = [[ ref_path.format(ET=et,ETA=eta) for eta in range(5)] for et in range(5)]\n",
    "ref_values = [[ {} for eta in range(5)] for et in range(5)]\n",
    "\n",
    "from saphyra.core import ReferenceReader\n",
    "for et_bin in range(5):\n",
    "    for eta_bin in range(5):\n",
    "        for op_name in op_names:\n",
    "            refObj = ReferenceReader().load(ref_paths[et_bin][eta_bin])\n",
    "            _pd = refObj.getSgnPassed(op_name)/refObj.getSgnTotal(op_name)\n",
    "            _fa = refObj.getBkgPassed(op_name)/refObj.getBkgTotal(op_name)\n",
    "            ref_values[et_bin][eta_bin][op_name] = {'pd':_pd, 'fa':_fa}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths.append(paths[4])\n",
    "ref_values.append(ref_values[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPatterns( path, etbin, etabin, kf, sort ):\n",
    "\n",
    "    pidname = 'el_lhmedium'\n",
    "    from kepler.pandas import load_hdf\n",
    "    import numpy as np\n",
    "    df = load_hdf(path)\n",
    "    df = df.loc[ ((df[pidname]==True) & (df.target==1.0)) | ((df.target==0) & (df['el_lhvloose']==False) ) ]\n",
    "    df = df.loc[ (df['trig_L2_cl_et']/1000  >= etbin[0]) & (df['trig_L2_cl_et']/1000  < etbin[1])]\n",
    "    df = df.loc[ (abs(df['trig_L2_cl_eta'])  >= etabin[0]) & (abs(df['trig_L2_cl_eta'])  < etabin[1])]\n",
    "\n",
    "  \n",
    "\n",
    "    # for new training, we selected 1/2 of rings in each layer\n",
    "    #pre-sample - 8 rings\n",
    "    # EM1 - 64 rings\n",
    "    # EM2 - 8 rings\n",
    "    # EM3 - 8 rings\n",
    "    # Had1 - 4 rings\n",
    "    # Had2 - 4 rings\n",
    "    # Had3 - 4 rings\n",
    "    prefix = 'trig_L2_cl_ring_%i'\n",
    "\n",
    "    # rings presmaple \n",
    "    presample = [prefix %iring for iring in range(8//2)]\n",
    "    # EM1 list\n",
    "    sum_rings = 8\n",
    "    em1 = [prefix %iring for iring in range(sum_rings, sum_rings+(64//2))]\n",
    "    # EM2 list\n",
    "    sum_rings = 8+64\n",
    "    em2 = [prefix %iring for iring in range(sum_rings, sum_rings+(8//2))]\n",
    "    # EM3 list\n",
    "    sum_rings = 8+64+8\n",
    "    em3 = [prefix %iring for iring in range(sum_rings, sum_rings+(8//2))]\n",
    "    # HAD1 list\n",
    "    sum_rings = 8+64+8+8\n",
    "    had1 = [prefix %iring for iring in range(sum_rings, sum_rings+(4//2))]\n",
    "    # HAD2 list\n",
    "    sum_rings = 8+64+8+8+4\n",
    "    had2 = [prefix %iring for iring in range(sum_rings, sum_rings+(4//2))]\n",
    "    # HAD3 list\n",
    "    sum_rings = 8+64+8+8+4+4\n",
    "    had3 = [prefix %iring for iring in range(sum_rings, sum_rings+(4//2))]\n",
    "    col_names = presample+em1+em2+em3+had1+had2+had3\n",
    "    rings = df[col_names].values.astype(np.float32)\n",
    "\n",
    "    def norm1( data ):\n",
    "        norms = np.abs( data.sum(axis=1) )\n",
    "        norms[norms==0] = 1\n",
    "        return data/norms[:,None]\n",
    "    \n",
    "    target = df['target'].values.astype(np.int16)\n",
    "    data_rings = norm1(rings)\n",
    "    \n",
    "    # This is mandatory\n",
    "    splits = [(train_index, val_index) for train_index, val_index in kf.split(data_rings,target)]\n",
    "    # split for this sort\n",
    "    x_train = [ data_rings[splits[sort][0]] ]\n",
    "    x_val   = [ data_rings[splits[sort][1]] ]\n",
    "    y_train = target [ splits[sort][0] ]\n",
    "    y_val   = target [ splits[sort][1] ]\n",
    "    \n",
    "    return x_train, x_val, y_train, y_val\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rerun references:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sp_func(pd, fa):\n",
    "    return np.sqrt(  np.sqrt(pd*(1-fa)) * (0.5*(pd+(1-fa)))  )\n",
    "\n",
    "#\n",
    "# Calculate sp, pd and fake given a reference\n",
    "# \n",
    "def calculate( y_train, y_val , y_op, ref, pd,fa,sp,thresholds, \n",
    "               pd_val,fa_val,sp_val,thresholds_val, \n",
    "               pd_op,fa_op,sp_op,thresholds_op ):\n",
    "\n",
    "    d = {}\n",
    "    def closest( values , ref ):\n",
    "        index = np.abs(values-ref)\n",
    "        index = index.argmin()\n",
    "        return values[index], index\n",
    "\n",
    "\n",
    "    d['pd_ref'] = ref['pd']\n",
    "    d['fa_ref'] = ref['fa']\n",
    "    d['sp_ref'] = ref['sp']\n",
    "    \n",
    "    op_total = len(y_op[y_op==1])\n",
    "    d['pd_ref_passed'] = int(ref['pd']*op_total)\n",
    "    d['pd_ref_total']  = op_total\n",
    "    op_total = len(y_op[y_op!=1])\n",
    "    d['fa_ref_passed'] = int(ref['fa']*op_total)\n",
    "    d['fa_ref_total']  = op_total\n",
    "\n",
    "    _, index = closest( pd_val, d['pd_ref'] )\n",
    "    val_total   = len(y_val[y_val==1])\n",
    "    d['pd_val'] = pd_val[index]  \n",
    "    d['pd_val_passed'] = int(val_total*float(pd_val[index]))\n",
    "    d['pd_val_total']  = val_total\n",
    "                   \n",
    "    val_total   = len(y_val[y_val!=1])\n",
    "    d['fa_val'] = fa_val[index]\n",
    "    d['fa_val_passed'] = int(val_total*float(fa_val[index]))\n",
    "    d['fa_val_total']  = val_total\n",
    "    \n",
    "    d['sp_val'] = sp_func(d['pd_val'], d['fa_val'])\n",
    " \n",
    "\n",
    "    # Train + Validation\n",
    "    _, index = closest( pd_op, d['pd_ref'] )\n",
    "    op_total = len(y_op[y_op==1])\n",
    "    d['pd_op'] = pd_op[index]\n",
    "    d['pd_op_passed'] = int(op_total*float(pd_op[index]))\n",
    "    d['pd_op_total'] = op_total\n",
    "    \n",
    "    op_total   = len(y_op[y_op!=1])\n",
    "    d['fa_op'] = fa_op[index]\n",
    "    d['fa_op_passed'] = int(op_total*float(fa_op[index]))\n",
    "    d['fa_op_total']  = op_total\n",
    "    \n",
    "    d['sp_op'] = sp_func(d['pd_op'], d['fa_op'])\n",
    "\n",
    "       \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(table_df, paths, refs, data_generator, kf, etbins, etabins, et_bin, eta_bin):\n",
    "        \n",
    "    tf.config.run_functions_eagerly(False)\n",
    "\n",
    "    bin_table = table_df.loc[ (table_df.et_bin==et_bin)&(table_df.eta_bin==eta_bin) ]\n",
    "    \n",
    "    et_bin_edges = (etbins[et_bin], etbins[et_bin+1])\n",
    "    eta_bin_edges = (etabins[eta_bin], etabins[eta_bin+1])\n",
    "    \n",
    "    for sort in tqdm( bin_table.sort.unique() , desc='Running (%d,%d)'%(et_bin,eta_bin), ncols=70 , total=len(bin_table.sort.unique())):\n",
    "        \n",
    "        x_train, x_val, y_train, y_val = data_generator(paths[et_bin][eta_bin], et_bin_edges, eta_bin_edges, kf, sort )\n",
    "        \n",
    "        view = bin_table.loc[bin_table.sort==sort]\n",
    "        \n",
    "        file_name = view.file_name.values[0]\n",
    "        tuned_idx = view.tuned_idx.values[0]\n",
    "        model_idx = view.model_idx.values[0]\n",
    "        tuned = gload(file_name)['tunedData'][tuned_idx]\n",
    "        model = model_from_json( json.dumps(tuned['sequence'], separators=(',', ':')) ) #custom_objects={'RpLayer':RpLayer} )\n",
    "        model.set_weights( tuned['weights'] )\n",
    "\n",
    "        y_pred     = model.predict( x_train, batch_size = 1024, verbose=0 )\n",
    "        y_pred_val = model.predict( x_val  , batch_size = 1024, verbose=0 )      \n",
    "        \n",
    "        # get vectors for operation mode (train+val)\n",
    "        y_pred_op = np.concatenate( (y_pred, y_pred_val), axis=0)\n",
    "        y_op = np.concatenate((y_train,y_val), axis=0)\n",
    "            \n",
    "        for idx, row in bin_table.loc[bin_table.sort==sort].iterrows():\n",
    "            \n",
    "            \n",
    "            ref = {'pd':refs[et_bin][eta_bin][row.op_name]['pd'], 'fa':refs[et_bin][eta_bin][row.op_name]['fa']}\n",
    "            ref['sp'] = sp_func(ref['pd'], ref['fa'])\n",
    "            train_total = len(y_train)\n",
    "            val_total = len(y_val)\n",
    "\n",
    "            # Here, the threshold is variable and the best values will\n",
    "            # be setted by the max sp value found in hte roc curve\n",
    "            # Training\n",
    "            fa, pd, thresholds = roc_curve(y_train, y_pred)\n",
    "            sp = sp_func(pd, fa)\n",
    "\n",
    "            # Validation\n",
    "            fa_val, pd_val, thresholds_val = roc_curve(y_val, y_pred_val)\n",
    "            sp_val = sp_func(pd_val, fa_val)\n",
    "            knee = np.argmax(sp_val)\n",
    "            table_df.at[idx, 'max_sp_val'] = sp_val[knee]\n",
    "            #table_df.at[idx, 'max_pd_val'] = pd_val[knee]\n",
    "            #table_df.at[idx, 'max_fa_val'] = fa_val[knee]\n",
    "\n",
    "\n",
    "            # Operation\n",
    "            fa_op, pd_op, thresholds_op = roc_curve(y_op, y_pred_op)\n",
    "            sp_op = sp_func(pd_op, fa_op)\n",
    "            knee = np.argmax(sp_op)\n",
    "            table_df.at[idx, 'max_sp_op'] = sp_op[knee]\n",
    "            #table_df.at[idx, 'max_pd_op'] = pd_op[knee]\n",
    "            #table_df.at[idx, 'max_fa_op'] = fa_op[knee]\n",
    "\n",
    "          \n",
    "            d = calculate( y_train, y_val , y_op, ref, \n",
    "                               pd, fa, sp, thresholds, \n",
    "                               pd_val, fa_val, sp_val, thresholds_val, \n",
    "                               pd_op,fa_op,sp_op,thresholds_op )\n",
    "          \n",
    "            #print(d['pd_ref_total'])\n",
    "         \n",
    "            for key in d.keys():\n",
    "                table_df.at[idx, key] = d[key]\n",
    "            #print('sort = %d, pid = %s, pd_ref = %1.4f -> pd_op = %1.4f, fa_op = %1.4f, idx=%d, tot=%d'%(sort, row.op_name, ref['pd'], \n",
    "            #                                                                                     d['pd_op'], d['fa_op'],idx, d['pd_ref_total']) )\n",
    "\n",
    "     \n",
    "    return table_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running (4,0): 100%|██████████████████| 10/10 [04:17<00:00, 25.71s/it]\n",
      "Running (4,1): 100%|██████████████████| 10/10 [02:43<00:00, 16.37s/it]\n",
      "Running (4,2): 100%|██████████████████| 10/10 [00:29<00:00,  2.99s/it]\n",
      "Running (4,3): 100%|██████████████████| 10/10 [01:51<00:00, 11.14s/it]\n",
      "Running (4,4): 100%|██████████████████| 10/10 [00:08<00:00,  1.12it/s]\n",
      "Running (5,3): 100%|██████████████████| 10/10 [01:59<00:00, 11.99s/it]\n",
      "Running (5,4):  60%|███████████▍       | 6/10 [00:04<00:02,  1.34it/s]"
     ]
    }
   ],
   "source": [
    "kf = StratifiedKFold(n_splits=10, random_state=512, shuffle=True)        \n",
    "best_inits_ext = best_inits.copy()\n",
    "etbins = [15, 20, 30, 40, 50, 100, 1000000]\n",
    "etabins = [0.0, 0.8, 1.37, 1.54, 2.37, 2.50]\n",
    "\n",
    "for et_bin in [4,5]:\n",
    "    for eta_bin in [0,1,2,3,4]:\n",
    "        rerun(best_inits_ext, paths, ref_values, \n",
    "              getPatterns, kf, etbins, etabins, et_bin, eta_bin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_inits_ext.to_csv( 'output/crossval/best_inits_v12_30bins.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
